{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07a8b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411982cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2501f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "# configure logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR)\n",
    "\n",
    "chrome_path = 'C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe'\n",
    "df = pd.read_excel(r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\NGB Yazaki\\US\\url_assignmnet.xlsx')\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.binary_location = chrome_path\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "retry_count = 3  # number of times to retry if an error occurs\n",
    "for i, row in df.iterrows():\n",
    "    url = row['{URL}']\n",
    "    name = row['{Name}']\n",
    "    for _ in range(retry_count):  # retry loop\n",
    "        try:\n",
    "            driver.delete_all_cookies()\n",
    "            driver.set_page_load_timeout(30)  # set a page load timeout of 30 seconds\n",
    "            driver.get(url)\n",
    "            time.sleep(10)\n",
    "            html = driver.page_source\n",
    "            with open(f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\NGB Yazaki\\\\US\\\\Assignment Page\\\\{name}.html', 'w', encoding='utf-8') as f:\n",
    "                f.write(html)\n",
    "                time.sleep(5)  # add a 5 seconds delay\n",
    "            break  # break out of the retry loop if successful\n",
    "        except (TimeoutException, WebDriverException) as e:\n",
    "            logging.error(f\"Error occurred while saving the page for {url}: {e}\")\n",
    "            print(f\"Error occurred while saving the page for {url}: {e}\")\n",
    "            time.sleep(10)  # add a 10 seconds delay before retrying\n",
    "    else:  # executed if the retry loop completes without success\n",
    "        logging.error(f\"Failed to save the page after {retry_count} retries for {url}\")\n",
    "        print(f\"Failed to save the page after {retry_count} retries for {url}\")\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c836ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal parse\n",
    "\n",
    "df_data = pd.DataFrame(columns=['Name', 'Data'])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    name = row['{Name}']\n",
    "    file_path = f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\NGB Yazaki\\\\US\\\\Patent Center Page\\\\{name}.html'\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    text = soup.get_text(separator='\\n')\n",
    "    text = text.replace('<div>', '<div> ') #added to add spaces \n",
    "    text = text.replace('\\n', ' \\n') # added to add spaces\n",
    "    df_data = pd.concat([df_data, pd.DataFrame({'Name': name, 'Data': text}, index=[0])])\n",
    "\n",
    "df_data.to_excel(r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\NGB Yazaki\\US\\scraped_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b958ddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e8bf5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parse with error messages on left column sucess or error\n",
    "df_data = pd.DataFrame(columns=['Name', 'Data', 'Status'])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    name = row['{Name}']\n",
    "    file_path = f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\NGB Yazaki\\\\US\\\\Patent Center Page\\\\{name}.html'\n",
    "    status = 'Success' # set default status as success\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        text = soup.get_text(separator='\\n')\n",
    "        text = text.replace('<div>', '<div> ')\n",
    "        text = text.replace('\\n', ' \\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing the file for {name}: {e}\")\n",
    "        text = ''\n",
    "        status = 'Error' # set status as error if there's an exception\n",
    "    \n",
    "    df_data = pd.concat([df_data, pd.DataFrame({'Name': name, 'Data': text, 'Status': status}, index=[0])])\n",
    "\n",
    "df_data.to_excel(r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\NGB Yazaki\\US\\scraped_data.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2efcbbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good morning\n"
     ]
    }
   ],
   "source": [
    "print(\"Good morning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c43e0036",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse with manual functality of choosing the excel file & it also shows error & sucess funtionality\n",
    "\n",
    "# Set the path to the Excel file containing the URLs manual funtionality\n",
    "url_file_path = r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\NGB Yazaki\\US\\url_assignmnet.xlsx'\n",
    "\n",
    "# Read the Excel file into a pandas dataframe\n",
    "df_urls = pd.read_excel(url_file_path)\n",
    "\n",
    "# Create an empty dataframe to store the parsed data\n",
    "df_data = pd.DataFrame(columns=['Name', 'Data', 'Status'])\n",
    "\n",
    "# Loop through the URLs in the dataframe and parse the HTML\n",
    "for i, row in df_urls.iterrows():\n",
    "    name = row['{Name}']\n",
    "    file_path = f'C:\\\\Users\\\\U6060174\\\\OneDrive - Clarivate Analytics\\\\Desktop\\\\Files\\\\NGB Yazaki\\\\US\\\\Assignment Page\\\\{name}.html'\n",
    "    status = 'Success' # set default status as success\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            html = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        text = soup.get_text(separator='\\n')\n",
    "        text = text.replace('<div>', '<div> ')\n",
    "        text = text.replace('\\n', ' \\n')\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while processing the file for {name}: {e}\")\n",
    "        text = ''\n",
    "        status = 'Error' # set status as error if there's an exception\n",
    "    \n",
    "    df_data = pd.concat([df_data, pd.DataFrame({'Name': name, 'Data': text, 'Status': status}, index=[0])])\n",
    "\n",
    "# Write the parsed data to an Excel file\n",
    "output_file_path = r'C:\\Users\\U6060174\\OneDrive - Clarivate Analytics\\Desktop\\Files\\NGB Yazaki\\US\\scraped_data_assignment.xlsx'\n",
    "df_data.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92c1e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29111\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad75a443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
